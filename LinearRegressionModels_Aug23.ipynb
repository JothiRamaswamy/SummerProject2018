{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "from allensdk.core.brain_observatory_cache import BrainObservatoryCache\n",
    "import allensdk.brain_observatory.receptive_field_analysis.visualization as rfvis\n",
    "import allensdk.brain_observatory.receptive_field_analysis.receptive_field as rf\n",
    "from allensdk.brain_observatory.receptive_field_analysis.eventdetection import detect_events\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allensdk.core.brain_observatory_cache import BrainObservatoryCache\n",
    "boc = BrainObservatoryCache(manifest_file='boc/manifest.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify output directories and cache results\n",
    "def cached(results_dir, results_name):\n",
    "    def _cache(func):\n",
    "        def func_wrapper(*args, **kwargs):\n",
    "            results_file = os.path.join(results_dir, results_name)\n",
    "            if not results_file.endswith(\".pkl\"):\n",
    "                results_file += \".pkl\"\n",
    "\n",
    "            if os.path.exists(results_file):\n",
    "                with open(results_file, \"rb\") as f:\n",
    "                    results = pickle.load(f)\n",
    "            else:\n",
    "                results = func(*args, **kwargs)\n",
    "                with open(results_file, \"wb\") as f:\n",
    "                    pickle.dump(results, f)\n",
    "\n",
    "            return results\n",
    "        return func_wrapper\n",
    "\n",
    "    return _cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids = [501474098,\n",
    "#  501847516]\n",
    "\n",
    "ids = [\n",
    "    501474098,\n",
    "    501847516,\n",
    "    505693621,\n",
    "    501773889,\n",
    "    511977695,\n",
    "    530047022,\n",
    "    503864409,\n",
    "    502383036,\n",
    "    540168837,\n",
    "    527583578,\n",
    "    528693630,\n",
    "    539540432,\n",
    "    510535700,\n",
    "    510656082,\n",
    "    511305590,\n",
    "    502665019,\n",
    "    530739576,\n",
    "    505811062,\n",
    "    501879034,\n",
    "    502483554,\n",
    "    506353473,\n",
    "    508220632,\n",
    "    537153918\n",
    "]\n",
    "\n",
    "num_experiments = len(ids)\n",
    "\n",
    "#note: 23 total, since each of the 6 cre-lines only has 1-3 different depths and 1-4 different brain locations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(i):\n",
    "    data = boc.get_ophys_experiment_data(i)\n",
    "    ts, dff = data.get_dff_traces()\n",
    "    T = ts.shape[0]\n",
    "    stimulus_table = data.get_stimulus_table(\"locally_sparse_noise\")\n",
    "    stimulus_template = data.get_stimulus_template(\"locally_sparse_noise\")[stimulus_table['frame'].values, :, :]\n",
    "    T_frames, H_px, W_px = stimulus_template.shape\n",
    "    stim_reshaped = stimulus_template.reshape((T_frames, -1))\n",
    "    x = np.zeros((T, H_px * W_px))\n",
    "    for t in range(T_frames):\n",
    "        x[stimulus_table.start[t]:stimulus_table.end[t]] = stim_reshaped[t]\n",
    "    y = dff.T\n",
    "    x -= x.mean()\n",
    "    x /= x.std()\n",
    "    y = dff.T\n",
    "    y -= y.mean(axis=0)\n",
    "    y /= y.std(axis=0)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, num_neurons = [], [], []\n",
    "for i in ids:\n",
    "    f = cached(\"data\", \"{}.pkl\".format(i))(get_data)\n",
    "    xi, yi = f(i)\n",
    "    X.append(xi)\n",
    "    Y.append(yi)\n",
    "    num_neurons.append(yi.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into test and train\n",
    "X_train = []\n",
    "X_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "for i in range(len(X)):\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X[i][:-10], Y[i][10:], test_size=0.80, random_state=42)\n",
    "    X_train.append(X_tr)\n",
    "    X_test.append(X_te)\n",
    "    y_train.append(y_tr)\n",
    "    y_test.append(y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "def linear_(X, Y):\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X, Y)\n",
    "    return lr\n",
    "\n",
    "def linear_with_neurons(X, Y, i):\n",
    "\n",
    "    neurons = np.column_stack((Y[:,:i], Y[:,i+1:]))\n",
    "    X_with_neurons = np.column_stack((X, neurons))\n",
    "        \n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_with_neurons, Y[:,i])\n",
    "\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fit linear regressions to each dataset\n",
    "all_lrs = []\n",
    "for j, idj in enumerate(ids):\n",
    "    fit = cached(\"results\", \"linear_{}_{}.pkl\".format(idj, j))(linear_)\n",
    "    lr = fit(X_train[j], y_train[j])\n",
    "    print(\"Dataset \", idj)\n",
    "    all_lrs.append(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute train losses for each dataset \n",
    "# train_scores = []\n",
    "# train_lps = []\n",
    "# for xtr, ytr, lrs in zip(X_train, y_train, all_lrs):\n",
    "#     T, N = ytr.shape\n",
    "#     score = 0\n",
    "#     lp = 0\n",
    "#     for i in range(N):\n",
    "#         score += (lrs[i].score(xtr, ytr[:,i]))\n",
    "#         lp += lrs[i].predict_log_proba(xtr)[np.arange(T), ytr[:,i].astype(int)].sum()\n",
    "#     train_scores.append(score / N)\n",
    "#     train_lps.append(lp / (T*N))\n",
    "\n",
    "    \n",
    "# # Compute test losses for each dataset\n",
    "# test_scores = []\n",
    "# test_lps = []\n",
    "# for xte, yte, lrs in zip(X_test, y_test, all_lrs):\n",
    "#     T, N = yte.shape\n",
    "#     score = 0\n",
    "#     lp = 0\n",
    "#     for i in range(N):\n",
    "#         score += (lrs[i].score(xte, yte[:,i]))\n",
    "#         lp += lrs[i].predict_log_proba(xte)[np.arange(T), yte[:,i].astype(int)].sum()\n",
    "#     test_scores.append(score / N)\n",
    "#     test_lps.append(lp / (T*N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit logistic regressions to each dataset\n",
    "all_lrs2 = []\n",
    "for j, idj in enumerate(ids):\n",
    "    Nj = Y[j].shape[1]\n",
    "    lrs_j = []\n",
    "    for i in range(Nj):\n",
    "        fit = cached(\"results\", \"linearwithneurons_{}_{}.pkl\".format(idj, i))(linear_with_neurons)\n",
    "        lr = fit(X_train[j], y_train[j], i)\n",
    "        lrs_j.append(lr)\n",
    "        print(\"Dataset \", j, \" neuron \", i)\n",
    "    all_lrs2.append(lrs_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute train losses for each dataset \n",
    "# train_scores2 = []\n",
    "# train_lps2 = []\n",
    "# for xtr, ytr, lrs in zip(X_train, y_train, all_lrs2):\n",
    "#     T, N = ytr.shape\n",
    "#     score = 0\n",
    "#     lp = 0\n",
    "#     for i in range(N):\n",
    "#         xtr_i = np.column_stack((xtr, ytr[:,:i], ytr[:,i+1:]))\n",
    "#         score += (lrs[i].score(xtr_i, ytr[:,i]))\n",
    "#         lp += lrs[i].predict_log_proba(xtr_i)[np.arange(T), ytr[:,i].astype(int)].sum()\n",
    "#     train_scores2.append(score / N)\n",
    "#     train_lps2.append(lp / (T*N))\n",
    "\n",
    "    \n",
    "# # Compute test losses for each dataset\n",
    "# test_scores2 = []\n",
    "# test_lps2 = []\n",
    "# for xte, yte, lrs in zip(X_test, y_test, all_lrs2):\n",
    "#     T, N = yte.shape\n",
    "#     score = 0\n",
    "#     lp = 0\n",
    "#     for i in range(N):\n",
    "#         xte_i = np.column_stack((xte, yte[:,:i], yte[:,i+1:]))\n",
    "#         score += (lrs[i].score(xte_i, yte[:,i]))\n",
    "#         lp += lrs[i].predict_log_proba(xte_i)[np.arange(T), yte[:,i].astype(int)].sum()\n",
    "#     test_scores2.append(score / N)\n",
    "#     test_lps2.append(lp / (T*N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corr_matrices(lrs):\n",
    "    corr = []\n",
    "\n",
    "    for lr in lrs:\n",
    "        corr.append(lr.coef_[448:])\n",
    "    corr = np.asarray(corr)\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,lrs in enumerate(all_lrs2):\n",
    "    lr = cached(\"correlations\", \"corr_matrix_{}_{}.pkl\".format(ids[i], i))(get_corr_matrices)\n",
    "    plt.figure()\n",
    "    plt.imshow(lr, cmap=\"RdBu\", vmin=.4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
